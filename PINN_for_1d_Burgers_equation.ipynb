{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f12043c",
   "metadata": {},
   "source": [
    "## Physics-Informed Neural Networks for 1D Burgers Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1a0541",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da56f1",
   "metadata": {},
   "source": [
    "Physics-Informed Neural Networks (PINNs), introduced by *Raissi et al. (2019)*, represent a transformative approach that seamlessly integrates physical laws into the training of neural networks. Unlike traditional neural networks that rely solely on data-driven methodologies, PINNs leverage the underlying governing equations (such as partial differential equations) directly within the loss function of the network.\n",
    "\n",
    "The central idea behind PINNs is to encode known physical constraints into the neural network's optimization process (basically the loss function), enabling the network not only to fit observational data but also to adhere closely to the underlying physics of the problem. This approach results in neural networks that are inherently consistent with physical laws, making PINNs highly effective in scenarios where data might be sparse or noisy.\n",
    "\n",
    "PINNs have found applications across various scientific and engineering domains, including fluid dynamics, solid mechanics, quantum mechanics, and more. Their primary advantage is the ability to yield physically accurate predictions with fewer or (no observed) training data points and improved interpretability.\n",
    "\n",
    "In general, PINNs are unsupervised, but they can be augmented with available data. However, employing PINNs also introduces specific challenges, such as ensuring adequate convergence during training, balancing the contribution of data-fitting (if data is present) and physics-based losses, and managing the computational complexity associated with evaluating the governing equations.\n",
    "\n",
    "In this notebook, we will demonstrate the practical implementation of PINNs using Pytorch Ligtning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637cce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Importing the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import seed_everything\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "#Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "  print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbab694",
   "metadata": {},
   "source": [
    "### Problem Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff12bcf",
   "metadata": {},
   "source": [
    "We consider the Burgers equation in one-dimension, which is a quasi-linear parabolic partial differential equation given as:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}, \\qquad 0 < x < 1, t > 0,\n",
    "\\end{equation}\n",
    "with initial condition\n",
    "$$\n",
    "    u(x,0) = sin(\\pi x), \\qquad 0< x < 1,\n",
    "$$\n",
    "and (homogenous) boundary conditions  \n",
    "$$\n",
    "    u(0,t) =  u(1,t) = 0,  \\qquad t > 0.\n",
    "$$\n",
    "\n",
    "> Note: The exact solution of the above Burger's equation can be obtained using the Hopf-Cole transformation, that transforms the equation into a linear heat equation.\n",
    "\n",
    "To solve using PINNs, define a residual $f(t, x)$ as:\n",
    "\\begin{equation}\n",
    "f := \\frac{\\partial u_{\\theta}}{\\partial t} + u_{\\theta} \\frac{\\partial u_{\\theta}}{\\partial x} - \\nu \\frac{\\partial^2 u_{\\theta}}{\\partial x^2},\n",
    "\\end{equation}\n",
    "where $ u_{\\theta}(x, t)$ is the velocity in the Burgers equation, now represented as a neural network (i.e is the output of a neural network).\n",
    "\n",
    "**Our goal is to let $f \\approx 0 $, so that our neural network output $u_{\\theta}$ will satisfy the Burgers equation (thus, a solution!!).**\n",
    "\n",
    "What makes a PINN different from a vanilla neural network is basically the loss function. In a PINN, the neural network parameters are trained by minimizing a composite loss function:\n",
    "$$\n",
    "MSE = MSE_u + MSE_f,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "MSE_u = \\frac{1}{N_u}\\sum_{i=1}^{N_u}\\left| u_{\\theta}(t_u^i, x_u^i) - u^i \\right|^2,\n",
    "$$\n",
    "and,\n",
    "$$\n",
    "MSE_f = \\frac{1}{N_f}\\sum_{i=1}^{N_f}\\left| f(t_f^i, x_f^i) \\right|^2.\n",
    "$$\n",
    "\n",
    "In this formulation:\n",
    "\n",
    "* $ (t_u^i, x_u^i, u^i)$ are observed data points with known values of the solution $ u $, such as the boundary points and/or data from a dataset (or exeperiments).\n",
    "\n",
    "* $ (t_f^i, x_f^i)$ are collocation points used to enforce the differential equation residual $ f(t,x) \\approx 0 $.\n",
    "\n",
    "> How do we compute the derivatives required for evaluating $ f(t,x) $? They are computed using automatic differentiation (or some known numerical differencing techniques, though this is less common), enabling the simultaneous enforcement of data-fitting and physical constraints.\n",
    "\n",
    "> In this Notebook, we take $\\nu = 1.0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878bf4a",
   "metadata": {},
   "source": [
    "### Configuration and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "733ede09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "  def __init__(self):\n",
    "    #The training parameters\n",
    "    self.num_epoch = 300\n",
    "    self.batch_size = 50\n",
    "    self.nu = 1.0\n",
    "    self.x0 = 0  #x0 :Left boundary point\n",
    "    self.xf = 1  #xf (x final): Right boundary point\n",
    "    self.layers = [2,125, 256, 125, 1]\n",
    "    self.spatial_resolution =  300\n",
    "    self.temporal_resolution = 150\n",
    "    self.init_cond = lambda x: torch.sin(torch.pi * x) #Defining the initial condition\n",
    "\n",
    "    #Optimizer\n",
    "    self.lr = 0.01\n",
    "    #self.weight_decay = 1e-3  #Regularization weight\n",
    "\n",
    "    #The learning rate scheduler\n",
    "    self.step_size = 75  #To decay after every, say 100 epochs\n",
    "    self.gamma = 0.5      #To reduce the learning rate by gamma (say, 1/2)\n",
    "\n",
    "    ##Model hyperparameters\n",
    "    self.hidden_layers = 125 #Hidden layers for trunk and branch\n",
    "\n",
    "\n",
    "\n",
    "    self.model_path = 'C:/Users/idris_oduola/Documents/Projects/RqPINN/dataset/pinn_burgers_model1d.pt'\n",
    "    self.checkpoint_dir = 'C:/Users/idris_oduola/Documents/Projects/RqPINN/dataset/checkpoint_burgers1d_pinn'\n",
    "\n",
    "\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f280890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatic differentiation in pytorch\n",
    "def dfx(f,x):\n",
    "  gouts = torch.ones([x.shape[0],1], dtype=torch.float, device = device)\n",
    "  return grad([f],[x],grad_outputs=gouts, create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aef44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(cfg):\n",
    "    x = torch.linspace(cfg.x0,cfg.xf, cfg.spatial_resolution).view(-1,1)\n",
    "    t = torch.linspace(0,1, cfg.temporal_resolution).view(-1,1)\n",
    "    #Now we create a mesh to obtain all possible coordinate points \n",
    "    x_mesh, t_mesh= torch.meshgrid(x.squeeze(1),t.squeeze(1))\n",
    "    print(f\"Shape of mesh: {x_mesh.shape}, {t_mesh.shape}\")\n",
    "\n",
    "    #Next we transform the mesh into a 2 column vector to obtain the coordinate points that will be passed in the neural network\n",
    "    x_stack=torch.hstack((x_mesh.transpose(1,0).flatten()[:,None],t_mesh.transpose(1,0).flatten()[:,None]))\n",
    "    print(f\"Shape after stacking: {x_stack.shape}\")\n",
    "\n",
    "    #Extracting the boundaries of the domain\n",
    "    bound1 = x_stack[0]; bound2 = x_stack[-1]\n",
    "    print(f\"Boundaries of the domain are: {bound1.shape}, {bound2.shape}\")\n",
    "\n",
    "    #Now we define the initial condition\n",
    "    left_X=torch.hstack((x_mesh[:,0][:,None],t_mesh[:,0][:,None])) # First column # The [:,None] is to give it the right dimension\n",
    "    left_u=cfg.init_cond(left_X[:,0]).unsqueeze(1)\n",
    "\n",
    "    #Boundary Conditions\n",
    "    #Bottom Edge: x = 0, t = [0,1]\n",
    "    bottom_X=torch.hstack((x_mesh[0,:][:,None],t_mesh[0,:][:,None])) # First row # The [:,None] is to give it the right dimension\n",
    "    bottom_u=torch.zeros(bottom_X.shape[0],1)\n",
    "\n",
    "    #The top Edge: x = 1, t =[0,1]\n",
    "    top_X=torch.hstack((x_mesh[-1,:][:,None],t_mesh[-1,:][:,None])) # Last row # The [:,None] is to give it the right dimension\n",
    "    top_u=torch.zeros(top_X.shape[0],1)\n",
    "\n",
    "    #Padding all together to implement the initial and boundary conditions\n",
    "    X_train=torch.vstack([left_X,bottom_X,top_X])\n",
    "    Y_train=torch.vstack([left_Y,bottom_Y,top_Y])\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
