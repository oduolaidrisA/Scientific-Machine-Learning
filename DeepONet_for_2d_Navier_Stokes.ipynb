{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba8dc51",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will solve for the vorticity evolution of the 2-d Navier-Stokes equation for viscous, incompressible fluid in vorticity form using a DeepONet. The branch consist of a UNet whose input is the initial condition, while the trunk's input are the spatial and temporal dimensions.\n",
    "\n",
    "Data information can be obtained from https://github.com/oduolaidrisA/Scientific-Machine-Learning/blob/main/data_generation.ipynb. From the data, there are 100 time snapshots of the vorticity evolution in $[0,50]$. The spatial domain considered is $(0,1)^2$ with 256 steps each in the $x$ and $y$ axis. Feeding these in a trunk-net with a feed-forward neural network implies there are $256 \\times 256 \\times 100 = 6,553,600$ points to be fed into the trunk. This is extremely huge and will be computationally expensive to process.\n",
    "\n",
    "A solution will be to use a separable deeponet $[1]$, where the trunk-net is a separable neural network. This basically means that instead of using a single feed-forward neural network in the trunk for the multi-dimensional coordinates, we can employ factorized coordinates and separate sub-networks for each on-dimensional domain. This means that for our problem the trunk consists of 3 sub-networks for the $x$, $y$ and $t$ domain. Thus, the total trunk input becomes $256+256+100 = 612$ points, which is ~$10,000 $ times decrease in the input. Thus, much more computationally efficient. It is important to note that this approach can only be utilized if the domain is separable, like the rectangular domain we have.\n",
    "\n",
    "Another approach is to include the spatial coordinates as channels in the branch input. This approach was used in $[2]$, where they utilized DeepONet for C02 sequestration. This means that the trunk input now consist of only the time-domain $t$, so the input to the trunk in our case is just $100$ points, which is ~$65,000$ decrease from the initial input we had. However, this approach will only work if the domain is separable and the inputs and outputs of the deepONet are of the same dimensions.\n",
    "\n",
    "For this notebook, we will utilize the second approach since we are solving for the vorticity evolution (which means the inputs and outputs are of same dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7bfd727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import seed_everything\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.set_float32_matmul_precision('high') # or'high'. This is to properly utilize Tensor Cores of my CUDA device ('NVIDIA RTX A6000')\n",
    "import h5py\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        #The File paths\n",
    "        self.data_path = 'C:/Users/idris_oduola/Documents/Projects/RqPINN/dataset/ns_data.h5' #Load data \n",
    "        self.model_path = 'C:/Users/idris_oduola/Documents/Projects/RqPINN/dataset/ns_model.pt'\n",
    "        self.checkpoint_dir = 'C:/Users/idris_oduola/Documents/Projects/RqPINN/dataset/checkpoint_ns'\n",
    "\n",
    "\n",
    "        #Model Parameters\n",
    "        self.channelS = 50\n",
    "        self.depth = 4\n",
    "        self.base_channel = 64\n",
    "\n",
    "        \n",
    "        #Optimizer\n",
    "        self.lr = 0.001\n",
    "        self.weight_decay = 1e-4 #Regularization weight\n",
    "        \n",
    "        #The training parameters\n",
    "        self.num_epoch = 100\n",
    "        self.batch_size = 20\n",
    "\n",
    "        #Learning rate scheduler\n",
    "        self.step_size = 20  #To decay after every, say 10 epochs\n",
    "        self.gamma = 0.5      #To reduce the learning rate by gamma (say, 1/2)\n",
    "\n",
    "        \n",
    "\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f0632",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2500b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(cfg.data_path, 'r') as file:\n",
    "    w_evolution = np.array(file['u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45071f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dim(data):\n",
    "    \"\"\"\n",
    "    A function to include the spatial dimensions\n",
    "\n",
    "    \"\"\"\n",
    "    B,H,W,T = data.shape\n",
    "    #The Spatial coordinates\n",
    "    x_coords = torch.linspace(0,1, steps = W).view(1,-1).expand(H,W) #(256,256)\n",
    "    y_coords = torch.linspace(0,1, steps = H).view(-1,1).expand(H,W) #(256,256)\n",
    "    #Stacking the coordinates\n",
    "    coord_stack = torch.tensor(np.stack([x_coords,y_coords], axis = -1)).float() #(256,256,2)\n",
    "    coord_stack = coord_stack.unsqueeze(0).unsqueeze(3) # (1,256,256,1,2)\n",
    "    coord_stack= coord_stack.repeat(B,1,1,T,1)\n",
    "    #Now we concatenate the data with the information on the spatial dimension\n",
    "    data1 = torch.tensor(data).unsqueeze(-1) # (B,256,256,T, 1)\n",
    "    new_data = torch.cat([data1, coord_stack], dim = -1) # (B,256,256,T,3)\n",
    "    assert new_data.shape[-1] == 3, print(f\"Loaded input variables successfully. Data now has size {new_data.shape}\")\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57836ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 256, 256, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_evolution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4960be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data,cfg):\n",
    "    \"\"\"\n",
    "    Prepares the train, val and test data\n",
    "    \n",
    "    \"\"\"\n",
    "    new_data = input_dim(data)\n",
    "    print(f\"Target is of Shape:{data.shape}\")\n",
    "\n",
    "    #Branch input\n",
    "    branch_input = new_data[:,:,:,0,:] #initial vorticity --> (B,256,256,3)\n",
    "    print(f\"Branch input is of Shape:{branch_input.shape}\")\n",
    "    training_points = data.shape[0]\n",
    "    dataset = TensorDataset(branch_input, torch.tensor(data))\n",
    "    train_set, val_set, test_set = random_split(dataset, [training_points - 100, 50, 50])\n",
    "\n",
    "    #DataLoader\n",
    "    train_loader = DataLoader(train_set, batch_size = cfg.batch_size, shuffle = True)\n",
    "    val_loader = DataLoader(val_set, batch_size = cfg.batch_size, shuffle = False)\n",
    "    test_loader = DataLoader(test_set, batch_size = cfg.batch_size, shuffle = False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e8b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target is of Shape:(500, 256, 256, 100)\n",
      "Branch input is of Shape:torch.Size([500, 256, 256, 3])\n",
      "DataLoaded Successfully\n"
     ]
    }
   ],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        self.train_loader, self.val_loader, self.test_loader= prepare_data(w_evolution,self.cfg)\n",
    "        print('DataLoaded Successfully!')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "data_module = DataModule(cfg)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34d95f",
   "metadata": {},
   "source": [
    "#### The DeepONet\n",
    "\n",
    "> As indicated earlier, we will use a UNet in the branch. However, it is important to note that any suitable network can be used in the branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477acaa9",
   "metadata": {},
   "source": [
    "##### The UNet\n",
    "We adopt similar UNet structure as in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4b7b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will be defining the convolutional block\n",
    "#This block will contain n_conv number of convolution layers, whose final layer downsamples the image resolution by 2 using strides\n",
    "class conv_blocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, n_conv = 3, mid_channels = None):\n",
    "        \"\"\"\n",
    "        n_conv*(conv --> batchnorm --> LeakyReLU)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        mid_channels = mid_channels or out_channels\n",
    "        def conv_block(in_ch, out_ch, strd):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=strd, padding = 1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.LeakyReLU(inplace = True)\n",
    "            )\n",
    "        layers = [conv_block(in_channels, mid_channels, 1)]\n",
    "        layers += [conv_block(mid_channels, mid_channels, 1) for _ in range(n_conv -2)]\n",
    "        layers += [conv_block(mid_channels, out_channels, stride)]\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176aa5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f1ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4948391",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Mandl, L., Goswami, S., Lambers, L., & Ricken, T. (2025). *Separable physics-informed DeepONet: Breaking the curse of dimensionality in physics-informed machine learning*. Computer Methods in Applied Mechanics and Engineering, 434, 117586.\n",
    "\n",
    "[2] Diab, W., & Al Kobaisi, M. (2024). *U-DeepONet: U-Net enhanced deep operator network for geologic carbon sequestration*. Scientific Reports, 14(1), 21298.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
