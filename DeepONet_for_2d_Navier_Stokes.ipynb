{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba8dc51",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will solve for the vorticity evolution of the 2-d Navier-Stokes equation for viscous, incompressible fluid in vorticity form using a DeepONet. The branch consist of a UNet whose input is the initial condition, while the trunk's input are the spatial and temporal dimensions.\n",
    "\n",
    "Data information can be obtained from https://github.com/oduolaidrisA/Scientific-Machine-Learning/blob/main/data_generation.ipynb. From the data, there are 100 time snapshots of the vorticity evolution in $[0,50]$. The spatial domain considered is $(0,1)^2$ with 256 steps each in the $x$ and $y$ axis. Feeding these in a trunk-net with a feed-forward neural network implies there are $256 \\times 256 \\times 100 = 6,553,600$ points to be fed into the trunk. This is extremely huge and will be computationally expensive to process.\n",
    "\n",
    "A solution will be to use a separable deeponet $[1]$, where the trunk-net is a separable neural network. This basically means that instead of using a single feed-forward neural network in the trunk for the multi-dimensional coordinates, we can employ factorized coordinates and separate sub-networks for each on-dimensional domain. This means that for our problem the trunk consists of 3 sub-networks for the $x$, $y$ and $t$ domain. Thus, the total trunk input becomes $256+256+100 = 612$ points, which is ~$10,000 $ times decrease in the input. Thus, much more computationally efficient. It is important to note that this approach can only be utilized if the domain is separable, like the rectangular domain we have.\n",
    "\n",
    "Another approach is to include the spatial coordinates as channels in the branch input. This approach was used in $[2]$, where they utilized DeepONet for C02 sequestration. This means that the trunk input now consist of only the time-domain $t$, so the input to the trunk in our case is just $100$ points, which is ~$65,000$ decrease from the initial input we had. However, this approach will only work if the domain is separable and the inputs and outputs of the deepONet are of the same dimensions.\n",
    "\n",
    "For this notebook, we will utilize the second approach since we are solving for the vorticity evolution (which means the inputs and outputs are of same dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7bfd727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import seed_everything\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.set_float32_matmul_precision('high') # or'high'. This is to properly utilize Tensor Cores of my CUDA device ('NVIDIA RTX A6000')\n",
    "import h5py\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0639efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        #The File paths\n",
    "        self.data_path = 'C:/Users/idris_oduola/Documents/Projects/RqPINN/dataset/ns_data.h5' #Load data \n",
    "        self.model_path = 'C:/Users/idris_oduola/Documents/Projects/RqPINN/dataset/ns_model_deep.pt'\n",
    "        self.checkpoint_dir = 'C:/Users/idris_oduola/Documents/Projects/RqPINN/dataset/checkpoint_ns'\n",
    "\n",
    "\n",
    "        #Model Parameters\n",
    "        self.lift_channel = 50\n",
    "        self.base_channel = 64\n",
    "        self.output_p = 32\n",
    "        self.layers = 5 \n",
    "\n",
    "        \n",
    "        #Optimizer\n",
    "        self.lr = 0.001\n",
    "        self.weight_decay = 1e-4 #Regularization weight\n",
    "        \n",
    "        #The training parameters\n",
    "        self.num_epoch = 100\n",
    "        self.batch_size = 20\n",
    "\n",
    "        #Learning rate scheduler\n",
    "        self.step_size = 20  #To decay after every, say 10 epochs\n",
    "        self.gamma = 0.5      #To reduce the learning rate by gamma (say, 1/2)\n",
    "\n",
    "        \n",
    "\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31f0632",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2500b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(cfg.data_path, 'r') as file:\n",
    "    w_evolution = np.array(file['u'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45071f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dim(data):\n",
    "    \"\"\"\n",
    "    A function to include the spatial dimensions\n",
    "\n",
    "    \"\"\"\n",
    "    B,H,W,T = data.shape\n",
    "    #The Spatial coordinates\n",
    "    x_coords = torch.linspace(0,1, steps = W).view(1,-1).expand(H,W) #(256,256)\n",
    "    y_coords = torch.linspace(0,1, steps = H).view(-1,1).expand(H,W) #(256,256)\n",
    "    #Stacking the coordinates\n",
    "    coord_stack = torch.tensor(np.stack([x_coords,y_coords], axis = -1)).float() #(256,256,2)\n",
    "    coord_stack = coord_stack.unsqueeze(0).unsqueeze(3) # (1,256,256,1,2)\n",
    "    coord_stack= coord_stack.repeat(B,1,1,T,1)\n",
    "    #Now we concatenate the data with the information on the spatial dimension\n",
    "    data1 = data.unsqueeze(-1) # (B,256,256,T, 1)\n",
    "    new_data = torch.cat([data1, coord_stack], dim = -1) # (B,256,256,T,3)\n",
    "    assert new_data.shape[-1] == 3, print(f\"Loaded input variables successfully. Data now has size {new_data.shape}\")\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57836ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_evolution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960be21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data,cfg):\n",
    "    \"\"\"\n",
    "    Prepares the train, val and test data\n",
    "    \n",
    "    \"\"\"\n",
    "    data = torch.tensor(data) #Make data a torch tensor\n",
    "\n",
    "    #Normalization\n",
    "    #data = (data - data.mean())/(data.std() + 1e-8) #The small value added is to avoid division by zero\n",
    "\n",
    "    new_data = input_dim(data)\n",
    "    print(f\"Target is of Shape:{data.shape}\")\n",
    "\n",
    "    #Branch input\n",
    "    branch_input = new_data[:,:,:,0,:] #initial vorticity --> (B,256,256,3)\n",
    "    print(f\"Branch input is of Shape:{branch_input.shape}\")\n",
    "    training_points = data.shape[0]\n",
    "    dataset = TensorDataset(branch_input, data)\n",
    "    train_set, val_set, test_set = random_split(dataset, [training_points - 100, 50, 50])\n",
    "\n",
    "    #DataLoader\n",
    "    train_loader = DataLoader(train_set, batch_size = cfg.batch_size, shuffle = True)\n",
    "    val_loader = DataLoader(val_set, batch_size = cfg.batch_size, shuffle = False)\n",
    "    test_loader = DataLoader(test_set, batch_size = cfg.batch_size, shuffle = False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        self.train_loader, self.val_loader, self.test_loader= prepare_data(w_evolution,self.cfg)\n",
    "        print('DataLoaded Successfully!')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "data_module = DataModule(cfg)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353317d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_module.test_dataloader()))\n",
    "w_init, w_fin = batch\n",
    "print(\"Initial Vorticity batch shape:\", w_init.shape)\n",
    "print(\"Final Vorticities batch shape:\", w_fin.shape)\n",
    "\n",
    "print(w_init.max(), w_init.min())\n",
    "print(w_fin.max(), w_fin.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34d95f",
   "metadata": {},
   "source": [
    "#### The DeepONet\n",
    "\n",
    "> As indicated earlier, we will use a UNet in the branch. However, it is important to note that any suitable network can be used in the branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477acaa9",
   "metadata": {},
   "source": [
    "##### The UNet\n",
    "We adopt similar UNet structure as in https://github.com/oduolaidrisA/Scientific-Machine-Learning/blob/main/unet_for_scientific_computing.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b7b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will be defining the convolutional block\n",
    "#This block will contain n_conv number of convolution layers, whose final layer downsamples the image resolution by 2 using strides\n",
    "class conv_blocks(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, n_conv = 3, mid_channels = None):\n",
    "        \"\"\"\n",
    "        n_conv*(conv --> batchnorm --> LeakyReLU)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        mid_channels = mid_channels or out_channels\n",
    "        def conv_block(in_ch, out_ch, strd):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=strd, padding = 1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.LeakyReLU(inplace = True)\n",
    "            )\n",
    "        layers = [conv_block(in_channels, mid_channels, 1)]\n",
    "        layers += [conv_block(mid_channels, mid_channels, 1) for _ in range(n_conv -2)]\n",
    "        layers += [conv_block(mid_channels, out_channels, stride)]\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define upsampling layer in the UNet.\n",
    "#upsampling is done using convTranspose\n",
    "#There is a number of convolution layers before each upsampling and the bottleneck.\n",
    "class up_layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.LeakyReLU(inplace = True)\n",
    "        )\n",
    "        def conv_block(in_ch, out_ch, strd):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=strd, padding = 1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.LeakyReLU(inplace = True)\n",
    "            )\n",
    "        layers = []\n",
    "        layers += [conv_block(in_channels, in_channels, 1) for _ in range(2)] #\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        \n",
    "\n",
    "    def forward(self, x1, x2): #x2 is the skip connection from the encoder\n",
    "        x1 = self.conv(x1)\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        #We need to add paddings to match x2 if needed\n",
    "        y_diff = x2.size()[2] - x1.size()[2]\n",
    "        x_diff = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [x_diff//2, x_diff - x_diff//2, y_diff//2, y_diff - y_diff//2])\n",
    "        x = torch.cat([x1,x2], dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f1ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.base_channel = cfg.base_channel\n",
    "        self.in_channel = cfg.lift_channel #Initial input channels coming from the lifting layer\n",
    "        self.out_channel = cfg.output_p #Final output channel\n",
    "\n",
    "        #The encoder block contains 3 conv blocks, we increase the number of channels by 2 as we downsize \n",
    "        self.encoder_block = nn.ModuleList([\n",
    "            conv_blocks(self.in_channel, self.base_channel, stride = 2, n_conv = 3, mid_channels = self.base_channel), #Output shape (n, f,128,128)\n",
    "            conv_blocks(self.base_channel, 2*self.base_channel, stride = 2, n_conv = 3), #Output shape (n, 2f,64,64)\n",
    "            conv_blocks(2*self.base_channel, 4*self.base_channel, stride = 2, n_conv = 3, mid_channels = 2*self.base_channel) #Output shape (n, 4f,32,32)\n",
    "        ])\n",
    "        \n",
    "        #The decoder block (There are 3 blocks here)\n",
    "        self.decoder_block = nn.ModuleList([\n",
    "            up_layer(4*self.base_channel, 2*self.base_channel), #Output shape (n, 4f,64,64)\n",
    "            up_layer(4*self.base_channel, self.base_channel), #Output shape (n, 2f, 128, 128)\n",
    "            up_layer(2*self.base_channel, self.in_channel) #Output shape (n, f, 256, 256)\n",
    "        ])\n",
    "    \n",
    "        #The final output convolution\n",
    "        self.final_conv = conv_blocks(2*self.in_channel, self.out_channel, stride = 1, n_conv = 3, mid_channels = self.base_channel) #Output shape (n, p, 256, 256)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"Initial x is of shape {x.shape}\")\n",
    "        res = []\n",
    "    \n",
    "        for i, down in enumerate(self.encoder_block): \n",
    "            res.append(x)\n",
    "            x = down(x)\n",
    "            #print(f\"During downsampling, x{i} is of shape {x.shape}\")\n",
    "    \n",
    "        for i, up in enumerate(self.decoder_block):\n",
    "            x2 = res.pop()\n",
    "            x = up(x, x2)\n",
    "            #print(f\"During upsampling, x{i} is of shape {x.shape}\")\n",
    "        x = self.final_conv(x)\n",
    "        #print(f\"x final is of shape {x.shape}\")\n",
    "        return x #self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the UNet outputs, uncomment the print functions in the unet class\n",
    "#sample = torch.randn(1,cfg.lift_channel,256,256)\n",
    "#mynet = UNet(cfg)\n",
    "#x = mynet(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88d956",
   "metadata": {},
   "source": [
    "#### Branch and Trunk Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f8543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we define the branch net\n",
    "#The branch net will contain a lifting layer, a UNet, and a branch and trunk multiplication\n",
    "class branch_net(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.lift = nn.Conv2d(in_channels = 3, out_channels = cfg.lift_channel, kernel_size = 1) #The Lift layer\n",
    "        self.unet = UNet(cfg)\n",
    "\n",
    "    def forward(self,x):\n",
    "        assert x.shape[1:] == (256,256,3), f\"Unexpected input shape: {x.shape}\"\n",
    "        x = x.permute(0,3,1,2) #Move the channels to have (batch, 3, 256,256)\n",
    "        x = self.lift(x) #Linear lifting layer --> (batch, f, 256,256)\n",
    "        #The UNet\n",
    "        x = self.unet(x) #UNet including the projection --> (batch, p, 256,256)\n",
    "\n",
    "        x = x.permute(0,2,3,1) # reorder for DeepOnet einsum product --> (batch, 256,256, p)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the Trunk net\n",
    "#Preparing data\n",
    "cfg.t_grid = torch.linspace(0,50,100).view(-1,1)\n",
    "#Normalization\n",
    "t_mean, t_std = cfg.t_grid.mean(), cfg.t_grid.std()\n",
    "cfg.t_grid = (cfg.t_grid - t_mean) / (t_std + 1e-6)\n",
    "class trunk_net(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "        self.out = cfg.output_p\n",
    "        self.hidden_dim = cfg.base_channel\n",
    "        self.layers = cfg.layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.linears = nn.ModuleList()\n",
    "\n",
    "        self.linears.append(nn.Linear(1, self.hidden_dim))\n",
    "        for _ in range(self.layers - 2):\n",
    "            self.linears.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        self.linears.append(nn.Linear(self.hidden_dim, self.out)) #Output is (100,p)\n",
    "\n",
    "    def forward(self,t):\n",
    "        a = t.float()\n",
    "        for linear in self.linears[:-1]:\n",
    "            a = self.activation(linear(a))\n",
    "        return self.linears[-1](a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99bd0c",
   "metadata": {},
   "source": [
    "#### Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class navier_deeponet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.branch = branch_net(cfg)\n",
    "        self.trunk = trunk_net(cfg)\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        x: shape (batch, 256,256,3)\n",
    "        t: (100, 1)\n",
    "\n",
    "        Output: (batch, 256,256,100)\n",
    "        \n",
    "        \"\"\"\n",
    "        t = self.cfg.t_grid.to(x.device)\n",
    "\n",
    "        #The branch net: (batch, 256,256,3) --> (batch, 256,256,p)\n",
    "        B_out = self.branch(x)\n",
    "\n",
    "        #The Trunk net: (100,1) --> (100,p)\n",
    "        T_out = self.trunk(t)\n",
    "\n",
    "        #Final output\n",
    "        output = torch.einsum(\"abcd, ed -> abce\",B_out,T_out) # (batch,256,256,100)\n",
    "\n",
    "        #We can add linear projection layers if we wish\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715227d6",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the model parameters\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlModel(pl.LightningModule):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model = navier_deeponet(self.cfg)\n",
    "        self.model.apply(init_weights)\n",
    "        self.metrics = {'train_loss':[], 'val_loss':[]}\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def loss_function(self,y_true,y_hat, beta = 0.5):\n",
    "\n",
    "        #A gradient loss is added to capture possible edges\n",
    "        grad_true_x = y_true[:, :, 1:, :] - y_true[:, :, :-1, :]\n",
    "        grad_true_y = y_true[:, 1:, :, :] - y_true[:, :-1, :, :]\n",
    "\n",
    "        grad_pred_x = y_hat[:, :, 1:, :] - y_hat[:, :, :-1, :]\n",
    "        grad_pred_y = y_hat[:, 1:, :, :] - y_hat[:, :-1, :, :]\n",
    "\n",
    "        direct_loss = F.mse_loss(y_hat, y_true)\n",
    "        return  direct_loss + beta*(F.mse_loss(grad_true_x, grad_pred_x) + F.mse_loss(grad_true_y, grad_pred_y))\n",
    "        \n",
    "    def _common_step(self, batch):\n",
    "        x,y = batch\n",
    "        y_hat = self(x)\n",
    "        assert y_hat.shape == y.shape, f\"Shape Mismatch! prediction shape:{y_hat.shape}, target shape: {y.shape}\"\n",
    "        loss = self.loss_function(y,y_hat, beta = 0.5)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        loss = self._common_step(batch)\n",
    "        self.log('train_loss', loss, on_step = True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.metrics['train_loss'].append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        loss = self._common_step(batch)\n",
    "        self.log('val_loss', loss, on_step =True, on_epoch = True, prog_bar = True, logger = True)\n",
    "        self.metrics['val_loss'].append(loss.item())\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        x,y = batch\n",
    "        pred = self(x) #or self.model(x)\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = self.cfg.lr, weight_decay = self.cfg.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = self.cfg.step_size, gamma = self.cfg.gamma)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate_on_test_set(self, loader):\n",
    "        self.eval()\n",
    "        losses = []; predictions = []; targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                x, y = batch\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                y_hat = self(x)\n",
    "                predictions.append(y_hat.cpu())\n",
    "                targets.append(y.cpu())\n",
    "\n",
    "                assert y_hat.shape == y.shape, f\"Prediction shape {y_hat.shape}, target shape {y.shape}\"\n",
    "\n",
    "                loss = self.loss_function(y,y_hat, beta = 0.5)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        avg_loss = sum(losses) / len(losses)\n",
    "        seis_hat = torch.cat(predictions,dim =0); seis_true = torch.cat(targets,dim =0)\n",
    "        print(f\"Average Loss on exact test set: {avg_loss:.4f}\")\n",
    "        return avg_loss, seis_hat, seis_true        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a50341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PlModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6731b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor = 'val_loss',\n",
    "    dirpath = cfg.checkpoint_dir,\n",
    "    filename = 'ns_deeponet-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k = 1,\n",
    "    mode = 'min',\n",
    "    save_weights_only = True\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = cfg.num_epoch,\n",
    "    callbacks = [checkpoint_callback],\n",
    "    accelerator = 'gpu',\n",
    "    devices = 1,\n",
    "    enable_progress_bar = True,\n",
    "    log_every_n_steps = 20,\n",
    "    deterministic = True\n",
    ")\n",
    "\n",
    "#Train the model\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9e3c5",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a25e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = sorted(Path(cfg.checkpoint_dir).glob(\"ns_deeponet-*.ckpt\"))[-1]\n",
    "model = PlModel.load_from_checkpoint(\n",
    "    checkpoint_path,\n",
    "    cfg=cfg \n",
    ")\n",
    "\n",
    "pred_loader = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed25c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgloss, preds, exact = model.evaluate_on_test_set(pred_loader)\n",
    "preds = preds.permute(0,3,1,2)\n",
    "exact = exact.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.suptitle(\"Predicted vs. Ground Truth vorticity Maps\", fontsize=16)\n",
    "n = 3\n",
    "for i in range(5):  # Plot 5 samples: pred + target\n",
    "    # Plot prediction\n",
    "    plt.subplot(5, 2, 2*i+1)\n",
    "    plt.imshow(preds[n, 30+i, :, :], aspect='auto', cmap='seismic', vmin=-1, vmax=1)\n",
    "    plt.title(f\"Prediction {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Plot corresponding ground truth\n",
    "    plt.subplot(5, 2, 2*i + 2)\n",
    "    plt.imshow(exact[n, 30+i, :, :], aspect='auto', cmap='seismic', vmin=-1, vmax=1)\n",
    "    plt.title(f\"Ground Truth {i}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbaa27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Animated Visualization for a sample in time\n",
    "%matplotlib qt\n",
    "\n",
    "sample_idx = 0\n",
    "\n",
    "u_pred = preds[sample_idx]    # Shape: (100, H, W)\n",
    "u_exact = exact[sample_idx]   # Shape: (100, H, W)\n",
    "\n",
    "# side-by-side figure setup\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "im_pred = axs[0].imshow(u_pred[0], cmap='seismic', vmin=-1, vmax=1)\n",
    "im_exact = axs[1].imshow(u_exact[0], cmap='seismic', vmin=-1, vmax=1)\n",
    "\n",
    "axs[0].set_title(\"Prediction at time step 0\")\n",
    "axs[1].set_title(\"Exact at time step 0\")\n",
    "\n",
    "#fig.colorbar(im_pred, ax=axs[0])\n",
    "fig.colorbar(im_exact, ax=axs[1])\n",
    "\n",
    "# Animation loop\n",
    "for i in range(u_pred.shape[0]):\n",
    "    im_pred.set_data(u_pred[i])\n",
    "    im_exact.set_data(u_exact[i])\n",
    "    axs[0].set_title(f\"Prediction at time step {i}\")\n",
    "    axs[1].set_title(f\"Exact at time step {i}\")\n",
    "    plt.pause(0.05)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4948391",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Mandl, L., Goswami, S., Lambers, L., & Ricken, T. (2025). *Separable physics-informed DeepONet: Breaking the curse of dimensionality in physics-informed machine learning*. Computer Methods in Applied Mechanics and Engineering, 434, 117586.\n",
    "\n",
    "[2] Diab, W., & Al Kobaisi, M. (2024). *U-DeepONet: U-Net enhanced deep operator network for geologic carbon sequestration*. Scientific Reports, 14(1), 21298.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
